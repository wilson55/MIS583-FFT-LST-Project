\documentclass[11pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{graphicx} % Required for inserting images
\usepackage{subcaption}
% Chinese support + UTF-8
\usepackage[UTF8]{ctex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{titling}
\usepackage{url}

%\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}

\setlength{\droptitle}{-5em}

\begin{document}

\begin{titlepage}
    \centering

   

    % --- School & Course Info ---
    {\Large 國立中山大學 資訊管理學系\\[6pt]
    MIS583 深度學習應用 期末專題報告 
     \\[0.5cm]}

    % --- Title ---
    {\LARGE \textbf{Toward Learnable Spectral Transforms: \\[6pt]
    MLP Approximation and Spectral Bias Analysis of the Fast Fourier Transform} \\[0.5 cm]}

    \section*{Executive Summary}

This project examines whether multilayer perceptrons (MLPs) can learn the Fast
Fourier Transform (FFT) as a spectral operator. Using ECG (structured) and
ESC-50 audio (non-stationary), we train two models—LST-MLP and φ-LST—to predict
FFT amplitude and phase. Amplitude is learnable, but phase shows strong
instability, discontinuities, and high-frequency error. Analysis reveals a clear
spectral bias favoring low frequencies and neuron receptive fields that resemble
Fourier bases. These findings clarify both the potential and the inherent
limitations of MLP-based spectral transforms.

\begin{figure}[h]
\centering
\includegraphics[width=0.55\linewidth]{pipeline.jpg}
\caption*{Overview of the Learnable Spectral Transform (LST) pipeline.}
\end{figure}

    \vfill
     % --- Group & Member ---
    {\large Group No.\ 11 \\[6pt]
    Team Member：王淵司（Wang Yuan Shih）Student ID：D134020001 \\[6pt]
    授課教師：楊惠芳教授}

\end{titlepage}
% ============================================
% Begin Main Body (after titlepage)
% ============================================
\setcounter{page}{1}     % Start page numbering from 1
\pagenumbering{arabic}

\newpage
\vspace{-0.8cm}
\section{Introduction}
\vspace{-0.5cm}
The Fast Fourier Transform (FFT) is one of the most influential algorithms in digital signal processing, enabling efficient conversion between time and frequency domains \cite{cooley1965fft, oppenheim2010dsp, stoica2005spectral}. Despite its elegance, the FFT relies on assumptions of stationarity and linearity that rarely hold in real-world applications. Biomedical signals such as ECG often exhibit morphological variability and transient disturbances, whereas environmental audio contains unpredictable bursts and wideband, non-stationary components. A fixed sinusoidal basis cannot adapt to such heterogeneity, motivating the study of learnable spectral transforms that can model frequency-domain behavior directly from data.

Recent advances illustrate a growing convergence between neural networks and spectral analysis. WaveNet demonstrates the ability to model raw audio using dilated convolutions \cite{oord2016wavenet}; SpectralNet uses deep learning to approximate spectral embeddings \cite{shaham2018spectralnet}; and Fourier Neural Operators (FNO) leverage Fourier kernels to solve parametric PDEs \cite{li2020fno, kovachki2021neuraloperator}. Parallel theoretical investigations show that modern neural networks possess an inherent \textit{spectral bias}, meaning they learn low-frequency components faster than high-frequency components \cite{rahaman2019spectral, misiakiewicz2021implicit, tancik2020fourier}. Furthermore, several works connect neural-network approximation theory with Fourier analysis \cite{basri2020fourier, yarotsky2017relu}.

Despite these developments, prior research seldom examines whether a neural network can \textit{explicitly learn the FFT operator} itself—i.e., to predict amplitude and phase for each frequency bin. This distinction is crucial: amplitude learning is relatively stable, whereas phase is discontinuous and highly sensitive to small perturbations, leading to significant reconstruction errors \cite{sitdikov2022phase}. Understanding these behaviors provides insight not only into spectral learning but also into the fundamental inductive biases of deep neural networks \cite{bietti2019inductive, jacot2018ntk}.

This study reformulates the FFT as a supervised learnable operator and systematically investigates whether a multilayer perceptron (MLP) can approximate the discrete Fourier transform across structured biomedical signals and non-stationary environmental audio. Beyond accuracy, we analyze the model's internal frequency behavior using receptive-field analysis, harmonic consistency tests, phase-stability evaluation, and frequency-locality mapping.

\noindent
\textbf{Research Questions}
\begin{itemize}
\vspace{-0.3cm}
    \item \textbf{RQ1.} Can an MLP learn accurate FFT amplitude and phase mappings across structured and non-stationary signals?
    \vspace{-0.5cm}
    \item \textbf{RQ2.} How does spectral bias manifest in amplitude and phase learning across low-, mid-, and high-frequency regions?
    \vspace{-0.5cm}
    \item \textbf{RQ3.} Do learned neurons form interpretable frequency-selective receptive fields analogous to sinusoidal bases?
    \vspace{-0.5cm}
    \item \textbf{RQ4.} How do dataset characteristics (ECG vs.\ ESC-50) influence learnability and error distribution?
\end{itemize}
%研究方法
\vspace{-0.8cm}
\section{Methodology}
This study adopts a complete spectral-learning pipeline implemented in
\texttt{mlp2fft\_20251210.py}, including FFT supervision, LST-MLP and
$\phi$-LST training, and multi-perspective spectral evaluation.

\subsection{Data Preparation and FFT Supervision}
Four signal sources are used to cover diverse spectral regimes:  
(1) MIT-BIH ECG (harmonic, structured),  
(2) ESC-50 audio (broadband, non-stationary),  
(3) synthetic chirp signals, and  
(4) synthetic harmonic signals.  
Each signal is segmented into windows of length $N\in\{64, 1024\}$. FFT
supervision uses the real-valued representation:
\[
F=\mathrm{rFFT}(x), \qquad Y=[\Re(F),\Im(F)].
\]
All spectral targets are standardized using training-set statistics.

\subsection{Baseline: Linear FFT Regression}
To verify pipeline correctness, two linear baselines are used: a closed-form
least-squares solution and \texttt{sklearn} LinearRegression. Their complex
relative error,
\[
\mathrm{RelErr}=\frac{\|F_{\text{pred}}-F_{\text{true}}\|_2}
{\|F_{\text{true}}\|_2},
\]
is near zero, confirming the validity of FFT preprocessing and the correctness
of supervised labels.

\subsection{Learnable Spectral Transform MLP (LST-MLP)}
The LST-MLP maps $x\in\mathbb{R}^N$ directly to real and imaginary FFT
components. The architecture consists of four fully connected layers with ReLU
activations and a bottleneck, followed by a linear output layer of size $2K$,
where $K=N/2+1$. The training loss is:
\[
L_{\text{MLP}}
=\mathrm{MSE}(\hat{F}_{\Re},F_{\Re})
+\mathrm{MSE}(\hat{F}_{\Im},F_{\Im}),
\]
and predictions are rescaled using stored statistics,
$F_{\text{pred}}=\mathrm{to\_complex}(\hat{Y}\sigma_y+\mu_y)$.

\subsection{$\phi$-LST: Stabilized Amplitude--Phase Learning}
Direct phase regression suffers from discontinuity and error spikes. The
$\phi$-LST decomposes phase into a quadrant classification and a residual phase
regression:
\[
\phi_{\text{hat}}=q_{\text{soft}}\frac{\pi}{2}+\Delta\phi_{\text{hat}},
\]
with complex reconstruction
\[
X_{\text{hat}}
=A_{\text{hat}}(\cos\phi_{\text{hat}}+j\sin\phi_{\text{hat}}).
\]
The total loss includes spectral error, iFFT reconstruction loss, phase loss,
quadrant cross-entropy, and amplitude loss:
\[
\mathcal{L}
=L_{\text{spec}}
+\lambda_{\text{rec}}L_{\text{rec}}
+\lambda_{\text{phase}}L_{\text{phase}}
+\lambda_{\text{quad}}L_{\text{quad}}
+\lambda_{\text{amp}}L_{\text{amp}}.
\]

\subsection{Evaluation Metrics}
We use four complementary metrics: complex relative error, amplitude MAE,
wrapped phase MAE, and time-domain reconstruction MSE (via iFFT). Wrapped phase
error is computed as:
\[
\phi_{\text{err}}
=\mathrm{wrap}(\phi_{\text{pred}}-\phi_{\text{true}}).
\]

\subsection{Frequency-Domain Interpretability Analyses}
To examine internal spectral behavior, we conduct three analyses:  
(1) neuron receptive-field matching with cosine/sine bases via cosine similarity;  
(2) frequency locality analysis through pure-tone injection;  
(3) harmonic consistency evaluation using multi-harmonic synthetic signals.  
These analyses reveal whether the network forms structured, frequency-selective
representations analogous to Fourier bases.

%結果與討論==========================
\vspace{-0.8cm}
\section{Results and Discussion}
\vspace{-0.2cm}
\subsection{Neuron RF Analysis}
\vspace{-0.2cm}
\begin{figure*}[t]
\centering

% --- (a) Neuron RF ---
\begin{subfigure}[t]{0.32\linewidth}
    \centering
    \includegraphics[width=\linewidth]{rf_all_neurons.jpg}
    \caption{Neuron RF Spectra}
    \label{fig:rf_all}
\end{subfigure}
\hfill
% --- (b) Locality ---
\begin{subfigure}[t]{0.32\linewidth}
    \centering
    \includegraphics[width=\linewidth]{locality_matrix.jpg}
    \caption{Amplitude Locality Matrix}
    \label{fig:locality}
\end{subfigure}
\hfill
% --- (c) Harmonic Consistency ---
\begin{subfigure}[t]{0.32\linewidth}
    \centering
    \includegraphics[width=\linewidth]{harmonic_consistency.jpg}
    \caption{Harmonic Consistency Example}
    \label{fig:harmonic}
\end{subfigure}

\caption{
Frequency-domain interpretability analysis.  
(a) Neuron receptive-field spectra reveal frequency-selective behavior and average low-frequency emphasis.  
(b) Frequency locality matrix shows strong diagonal dominance, confirming that low-frequency tones yield localized outputs while higher frequencies exhibit stronger mixing.  
(c) Harmonic consistency test demonstrates that LST approximates fundamental and harmonic peaks but underestimates higher-order components.
}
\label{fig:interpretability}
\end{figure*}
The three interpretability analyses in Fig.~\ref{fig:interpretability} collectively
demonstrate that the LST-MLP develops clear frequency-selective structure.  
First, the individual neuron RF spectra in Fig.~\ref{fig:interpretability}(a)
show that first-layer neurons consistently exhibit stronger responses in the low-
and mid-frequency ranges, with a sharp magnitude drop beyond the Nyquist midpoint.
This pattern resembles learned Fourier-like receptive fields and reflects the
spectral bias of neural networks.  

Second, the frequency locality matrix in Fig.~\ref{fig:interpretability}(b)
confirms that each input tone predominantly activates a localized output frequency
region, indicating near-orthogonality analogous to cosine/sine bases.  
The slight spreading observed at higher frequencies highlights the limited
learnability of broadband components.  

Finally, the harmonic consistency test in
Fig.~\ref{fig:interpretability}(c) shows that the model reliably recovers
fundamental and low-order harmonics while underestimating higher-order peaks.
This degradation is consistent with the reduced high-frequency selectivity seen
in the RF spectra.  

Taken together, Fig.~\ref{fig:interpretability}(a)--(c) verify that the
LST-MLP internalizes interpretable Fourier-like structures and satisfies the
objectives of the RF Matching analysis.
\vspace{-0.5cm}
\subsection{Phase Instability}
\vspace{-0.2cm}
\begin{figure*}[t]
\centering

% ---------------- (a) ECG Amplitude Error ----------------
\begin{subfigure}[t]{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{amp_error_ecg.jpg}
    \caption{Frequency-wise amplitude error (MIT-BIH ECG).}
    \label{fig:phase_instability_ecg}
\end{subfigure}
\hfill
% ---------------- (b) ESC-50 Amplitude Error ----------------
\begin{subfigure}[t]{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{amp_error_esc50.jpg}
    \caption{Frequency-wise amplitude error (ESC-50 audio).}
    \label{fig:phase_instability_esc50}
\end{subfigure}
\hfill
% ---------------- (c) Time-domain Reconstruction ----------------
\begin{subfigure}[t]{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{reconstruction_phi.jpg}
    \caption{Time-domain reconstruction comparing original vs.\ $\phi$-LST output.}
    \label{fig:phase_instability_recon}
\end{subfigure}

\caption{
Phase instability analysis across datasets.
(a) ECG amplitude errors decrease sharply across frequencies, indicating stable harmonic structure. 
(b) ESC-50 exhibits large and irregular amplitude errors, implying strong amplitude–phase coupling and unstable phase learning. 
(c) Time-domain reconstruction demonstrates how small phase deviations produce significant waveform distortion even when amplitude estimates are accurate.
}
\label{fig:phase_instability}
\end{figure*}

Phase prediction exhibits substantially higher instability than amplitude
prediction, and the three analyses in Fig.~\ref{fig:phase_instability}(a)--(c)
highlight the underlying reasons. First, although amplitude errors decrease
rapidly across frequency bins for structured signals such as MIT-BIH ECG
(Fig.~\ref{fig:phase_instability}(a), left), the same trend does not hold for
non-stationary audio. The ESC-50 curves in Fig.~\ref{fig:phase_instability}(a), right,
remain flat and comparatively large across all frequencies. This contrast
indicates that the model fails to exploit harmonic regularity when phase varies
rapidly, resulting in poor amplitude–phase coupling and unstable phase learning.

Second, the time-domain reconstruction results in
Fig.~\ref{fig:phase_instability}(c) reveal the practical consequence of phase
error. Even when amplitude is predicted reasonably well, small phase deviations
accumulate across frequency bins and produce noticeable waveform distortions.
The φ-LST reconstruction partially mitigates these discrepancies but still
exhibits temporal misalignment and oscillatory drift, demonstrating that phase
remains the dominant source of reconstruction error.

Together, Fig.~\ref{fig:phase_instability}(a)--(c) demonstrate that phase
components are fundamentally more difficult to learn than amplitude: they lack
spatial smoothness, contain discontinuities at \( \pm \pi \), and amplify
frequency-wise prediction noise when mapped back to the time domain. These
properties explain the strong phase instability observed in the LST-MLP and
motivate the need for quadrant-aware phase decomposition in φ-LST.
\vspace{-0.5cm}
\subsection{Dataset Effects}
\vspace{-0.2cm}
The role of dataset structure becomes evident when comparing the results across
MIT-BIH ECG and ESC-50 audio. As shown in the Phase Instability analysis in
Fig.~\ref{fig:phase_instability}(a)--(b), ECG signals exhibit rapidly decreasing
amplitude error across frequency bins due to their stable and harmonic spectral
structure. In contrast, ESC-50 audio produces consistently high and irregular
errors, reflecting its broadband and transient-rich nature. These results
indicate that harmonic signals align well with the inductive bias of MLP-based
spectral models, whereas broadband signals induce strong spectral mixing and
unstable amplitude–phase coupling.

To further illustrate this effect, the harmonic consistency analysis in
Fig.~\ref{fig:interpretability}(c) shows that the model accurately reconstructs
fundamental and low-order harmonics for ECG-like signals but systematically
underestimates higher harmonics. Signals lacking harmonic regularity, such as
ESC-50 audio, do not exhibit such stable spectral patterns, highlighting their
greater learning difficulty.

Together, these observations confirm that dataset characteristics—harmonicity,
stationarity, and spectral concentration—directly influence spectral
learnability and explain the pronounced performance gap between ECG and
ESC-50.

\vspace{-0.5cm}
\subsection{Neuron Receptive Fields}
\vspace{-0.2cm}
As shown in the RF analysis in Fig.~\ref{fig:interpretability}(a) and
Fig.~\ref{fig:interpretability}(b), first-layer weight vectors develop
frequency-selective receptive fields that closely resemble low-frequency
cosine/sine bases. Most neurons exhibit smooth, band-limited spectra with
energy concentrated in the low- and mid-frequency ranges. The histogram of
dominant frequencies in Fig.~\ref{fig:interpretability}(c) further shows that
low-frequency selective neurons are abundant, whereas high-frequency neurons are
sparse, indicating a gradual degradation of representation quality toward the
Nyquist region. This emergent Fourier-like organization is consistent with
theoretical predictions in~\cite{rahaman2019spectral, misiakiewicz2021implicit}.
\vspace{-0.5cm}
\section{Conclusion}
\vspace{-0.5cm}
This study provides the first systematic examination of the feasibility and
limitations of treating the Fast Fourier Transform as a learnable operator
within multilayer perceptrons. While earlier sections established that amplitude
mappings are reliably learnable, our results also reveal persistent phase
instability and pronounced frequency-dependent error patterns that expose
fundamental constraints of neural spectral representations.

The findings provide clear answers to the four research questions. RQ1 asked
whether an MLP can learn accurate amplitude and phase mappings across
heterogeneous signals. Empirically, the model consistently learns amplitude
patterns but fails to stably approximate phase, especially at higher
frequencies, indicating that FFT approximation is only partially achievable with
standard real-valued MLPs. RQ2 examined the manifestation of spectral bias. Our
frequency-resolved analyses show a strong and systematic low-frequency bias:
errors increase monotonically toward the Nyquist region, and phase spikes align
with theoretical predictions of high-frequency underfitting. RQ3 explored the
interpretability of internal spectral representations. Receptive-field matching
demonstrates that first-layer neurons spontaneously develop Fourier-like filters
in the low-frequency range, with increasingly diffuse and mixed responses at
higher frequencies, revealing an emergent but frequency-limited spectral
structure. RQ4 addressed dataset effects, showing that structured,
quasi-periodic ECG signals are substantially easier for the model to learn than
broadband, non-stationary ESC-50 audio, highlighting the strong dependence of
spectral learnability on signal complexity and distributional properties.

Together, these results clarify both the promise and the inherent limitations of
MLP-based FFT approximation. While neural networks can partially internalize
Fourier structure, their ability to serve as general-purpose spectral transforms
is fundamentally constrained by spectral bias, phase discontinuity, and
data-dependent frequency complexity. Future work should explore complex-valued
architectures, phase-aware loss functions, and adaptive basis learning to
overcome these limitations and move toward more robust, fully learnable spectral
transform operators.

\newpage
\section*{Resources}

\begin{itemize}
    \item MIT-BIH Arrhythmia Database  
    \url{https://physionet.org/content/mitdb/1.0.0/}
    \item ESC-50 Environmental Sound Dataset  
    \url{https://github.com/karoldvl/ESC-50}
    \item Reference implementation: Fourier Neural Operator  
    \url{https://github.com/neuraloperator/neuraloperator}
\end{itemize}
\bibliographystyle{IEEEtran}
\bibliography{refs}
\end{document}
